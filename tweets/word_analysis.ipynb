{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_words = pd.read_csv('nlp_parsed_words.csv')\n",
    "pos = nlp_words['pos'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NNP', 0.17751241982540225), ('NN', 0.10650688703678075), ('IN', 0.09290227101344081), ('DT', 0.07517418822778482), ('JJ', 0.06307492000192051), ('.', 0.05903335771639021), ('RB', 0.04335288685037747), ('PRP', 0.04010495070197785), ('NNS', 0.037249591183689146), ('VB', 0.03604926695493277), ('CC', 0.031222551409180645), ('VBP', 0.02603997503325604), ('VBZ', 0.024319981020755724), ('TO', 0.0230066850998811), ('VBG', 0.021687740594400557), ('VBD', 0.021603011825311873), (':', 0.019645777259363235), ('VBN', 0.01833248133848861), ('PRP$', 0.0164119625724784), ('MD', 0.013596143146431082), ('CD', 0.011864851964718941), ('NNPS', 0.0068997460961219646), ('WP', 0.004456733254064863), ('WRB', 0.0036631071169341742), ('RP', 0.003417393686576986), ('JJR', 0.003191450302340491), ('WDT', 0.002996574133436514), ('(', 0.0029400882873773905), (')', 0.002917493948953741), ('JJS', 0.0024684314727837072), ('EX', 0.0015222935512933847), ('RBR', 0.0013895518130544438), ('POS', 0.0009743808445198844), ('``', 0.0008698820293105055), (\"''\", 0.0007371402910715647), ('$', 0.0006580601065887915), ('RBS', 0.0006015742605296677), ('PDT', 0.0005620341682882812), (',', 0.00045188676847298986), ('FW', 0.0002485377226601444), ('UH', 0.0002146462150246702), ('WP$', 9.320164599755416e-05), ('SYM', 3.389150763547424e-05)]\n"
     ]
    }
   ],
   "source": [
    "pos_hist = {}\n",
    "for part in pos:\n",
    "    pos_hist[part] = len(nlp_words[nlp_words.pos == part])/len(nlp_words)\n",
    "\n",
    "sorted_hist = sorted(pos_hist.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_hist)\n",
    "\n",
    "# so by selecting NNP, NN, and NNS we can already cover about 30% of the words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 113752 nouns\n",
      "    505 nouns about China\n",
      "    349 nouns about Russia\n",
      "    1317 nouns about Mexico\n",
      "    69 nouns about Canada\n",
      "    320 nouns about Korea\n",
      "    741 nouns about Clinton\n",
      "    928 nouns about Democrats\n",
      "    474 nouns about Republicans\n",
      "    4608 nouns about himself\n",
      "    424 nouns about family\n",
      "    3386 nouns about government\n",
      "    2684 nouns about media\n",
      "    1269 nouns about states\n",
      "    132 nouns about social_media\n",
      "    371 nouns about Obama\n",
      "    795 nouns about FBI\n",
      "    193 nouns about rivals\n",
      "    547 nouns about MiddleEast\n",
      "    411 nouns about Ukraine_scandal\n",
      "    1462 nouns about economy\n",
      "    234 nouns about disaster\n",
      "there are 92533 unassigned nouns\n",
      "[('Retweet', 2621), ('’', 1388), ('”', 1224), ('people', 1007), ('t', 833), ('“', 788), ('Thank', 574), ('time', 524), ('years', 517), ('%', 430), ('today', 407), ('House', 391), ('job', 364), ('s', 299), ('way', 294), ('North', 280), ('Witch', 270), ('Hunt', 269), ('New', 256), ('deal', 254), ('history', 243), ('year', 236), ('day', 231), ('Today', 222), ('Security', 219), ('..', 207), ('Congratulations', 204), ('White', 204), ('honor', 204), ('Trade', 203), ('National', 202), ('nothing', 196), ('things', 196), ('Administration', 195), ('Collusion', 193), ('countries', 188), ('world', 183), ('meeting', 181), ('others', 177), ('Election', 175), ('Governor', 173), ('fact', 172), ('Party', 164), ('Crime', 163), ('election', 162), ('campaign', 162), ('story', 161), ('crime', 159), ('night', 157), ('Tariffs', 157), ('John', 156), ('work', 153), ('Justice', 147), ('Senator', 146), ('vote', 144), ('Report', 144), ('People', 137), ('trade', 135), ('book', 133), ('Military', 131), ('numbers', 130), ('General', 130), ('THE', 129), ('record', 125), ('law', 125), ('Minister', 125), ('everyone', 124), ('women', 124), ('Prime', 124), ('thing', 122), ('place', 120), ('immigration', 118), ('Court', 118), ('tonight', 114), ('World', 111), ('Endorsement', 110), ('Billion', 110), ('James', 108), ('yesterday', 107), ('YOU', 106), ('Day', 105), ('family', 104), ('Joe', 104), ('man', 104), ('Vets', 103), ('anything', 103), ('families', 103), ('support', 102), ('First', 101), ('order', 101), ('Secretary', 100), ('Market', 98), ('South', 97), ('A', 97), ('security', 96), ('Congressman', 95), ('call', 95), ('something', 95), ('win', 95), ('American', 94), ('Times', 94), ('Federal', 93), ('men', 92), ('tomorrow', 91), ('Supreme', 90), ('investigation', 89), ('stories', 88), ('Nation', 88), ('business', 88), ('Stock', 88), ('information', 87), ('workers', 87), ('problem', 87), ('week', 87), ('God', 86), ('Japan', 86), ('leaders', 86), ('Adam', 85), ('Cuts', 85), ('Amendment', 85), ('collusion', 85), ('morning', 85), ('case', 84), ('Borders', 83), ('–', 83), ('ratings', 83), ('Immigration', 82), ('unemployment', 82), ('government', 82), ('Big', 82), ('lives', 81), ('Wow', 81), ('reason', 81), ('future', 80), ('home', 80), ('days', 79), ('NO', 79), ('friend', 79), ('VOTE', 78), ('laws', 78), ('iPhone', 78), ('THANK', 77), ('Dossier', 77), ('Jobs', 77), ('everything', 76), ('person', 76), ('Hoax', 75), ('success', 75), ('https', 75), ('Department', 75), ('help', 74), ('statement', 74), ('rate', 74), ('Campaign', 74), ('U.S', 74), ('power', 73), ('No', 73), ('Obstruction', 73), ('Michael', 73), ('level', 73), ('end', 73), ('part', 73), ('decision', 73), ('dbongino', 73), ('Law', 72), ('Post', 72), ('life', 72), ('office', 72), ('seanhannity', 72), ('team', 72), ('Fed', 71), ('anyone', 71), ('system', 71), ('guy', 70), ('months', 70), ('safety', 70), ('attack', 69), ('TomFitton', 69), ('France', 69), ('number', 68), ('words', 68), ('relationship', 68), ('Scavino45', 68), ('Bob', 68), ('care', 68), ('state', 68), ('Nothing', 67), ('service', 67), ('lot', 67), ('crowd', 67), ('Committee', 66), ('Enjoy', 66), ('Chuck', 66), ('LouDobbs', 66), ('Vote', 66), ('Mark', 65), ('members', 65), ('victory', 65), ('P.M.', 65), ('Deal', 65), ('NOT', 65), ('nation', 65), ('November', 64), ('leadership', 64), ('show', 63), ('friends', 63), ('JudicialWatch', 63), ('meetings', 63), ('Andrew', 63), ('votes', 63), ('Chairman', 62), ('president', 62), ('Farmers', 62), ('Robert', 62), ('Director', 62), ('millions', 61), ('Puerto', 61), ('Rico', 61), ('…', 60), ('Radical', 59), ('decades', 59), ('Patrol', 59), ('Office', 59), ('process', 58), ('sources', 58), ('evidence', 58), ('race', 58), ('right', 58), ('borders', 58), ('McCabe', 58), ('Peter', 58), ('prayers', 58), ('reporting', 57), ('Jim_Jordan', 56), ('one', 56), ('point', 55), ('Rep.', 55), ('farmers', 55), ('game', 55), ('BIG', 55), ('press', 55), ('Government', 54), ('USMCA', 54), ('Dan', 54), ('interview', 54), ('enforcement', 53), ('NOTHING', 53), ('prices', 53), ('Jeff', 53), ('Kavanaugh', 53)]\n"
     ]
    }
   ],
   "source": [
    "nouns = nlp_words[nlp_words['pos'].isin(['NNP', 'NN', 'NNS'])]\n",
    "print(f\"there are {len(nouns)} nouns\")\n",
    "\n",
    "def freq_analysis(to_analyze):\n",
    "    nouns_by_freq = {}\n",
    "    for noun in to_analyze['word']:\n",
    "        try: \n",
    "            nouns_by_freq[noun] += 1\n",
    "        except:\n",
    "            nouns_by_freq[noun] = 1\n",
    "\n",
    "    sorted_nouns = sorted(nouns_by_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(sorted_nouns[0:250])      \n",
    "\n",
    "keywords_by_topic = {}\n",
    "keywords_by_topic['China'] = [\"China\", \"Xi\", \"Jinping\", \"Hong Kong\", \"Beijing\", \"Shanghai\", \"Chinese\", \"Taiwan\"]\n",
    "keywords_by_topic['Russia'] = [\"Vladimir\", \"Putin\", \"Russia\", \"Moscow\", \"Soviet\", \"Siberia\", \"Russian\"]\n",
    "keywords_by_topic['Mexico'] = [\"Mexico\", \"Border\", \"immigrants\", \"mexican\", \"Wall\", \"Obrador\", \"Mexican\", \"DACA\", \"WALL\", \"border\"]\n",
    "keywords_by_topic['Canada'] = [\"Canada\", \"Quebec\", \"Toronto\", \"Montreal\", \"Ottawa\", \"JustinTrudeau\", \"Trudeau\"]\n",
    "keywords_by_topic['Korea'] = [\"Korea\", \"Kim\", \"Rodman\", \"Pyongyang\", \"Seoul\", \"Korean\"]\n",
    "keywords_by_topic['Clinton'] = [\"Hillary\", \"Clinton\", \"Bill\", \"Crooked\", \"Watergate\", \"Arkansas\"]\n",
    "keywords_by_topic['Democrats'] = [\"Dems\", \"Democrats\", \"Nancy\", \"SpeakerPelosi\", \"Left\", \"Leftist\", \"Socialist\", \"Socialism\", \"Pelosi\", \"Democrat\", \"Democratic\", \"democratics\", \"democrat\", \"DNC\", \"Schumer\", \"Libs\", \"Liberal\", \"Liberals\"]\n",
    "keywords_by_topic['Republicans'] = [\"GOP\", \"Republicans\", \"REPUBLICANS\", \"Right\", \"republicans\", \"Republican\", \"republican\", \"McConnell\", \"GOPChairwoman\", \"RNC\", \"VP\", \"Pence\"]\n",
    "keywords_by_topic['himself'] = ['Donald', 'Trump', 'POTUS', 'MAGA', 'realDonaldTrump', 'WhiteHouse', 'President', 'MAKE', 'AMERICA', 'GREAT', 'AGAIN', \"Great\", \"HouseGOP\"]\n",
    "keywords_by_topic['family'] = [\"FLOTUS\", \"Melania\", \"Ivanka\", \"Don\", \"IvankaTrump\", \"DonaldJTrumpJr\"]\n",
    "keywords_by_topic['government'] = [\"Senate\", \"Congress\", \"USA\", \"U.S.\", \"United\", \"America\", \"States\", \"Country\", \"State\", 'country', \"Washington\"]\n",
    "keywords_by_topic['media'] = [\"Fox\", \"CNN\", \"News\", \"news\", \"media\", \"Media\", \"foxandfriends\", \"FoxNews\", \"Fake\", 'fake', \"NEWS\", \"FAKE\"]\n",
    "keywords_by_topic['states'] = [\"California\", \"Texas\", \"Florida\", \"Virginia\", \"Iowa\", \"Hampshire\", \"Carolina\", \"Oregon\", \"Dakota\", \"Utah\", \"Pennsylvania\", \"Oklahoma\", \"Nevada\", \"Arizona\", \"Montana\", \"Idaho\", \"Nebraska\", \"Michigan\", \"Illinois\", \"Indiana\", \"Ohio\", \"Vermont\", \"Maine\", \"York\", \"Tennessee\", \"Mississippi\", \"Louisiana\", \"Alabama\", \"Georgia\", \"Delaware\"]\n",
    "keywords_by_topic['social_media'] = [\"Twitter\", \"Facebook\", \"Zuckerberg\", \"Dorsey\"]\n",
    "keywords_by_topic['Obama'] = [\"Barack\", \"Obama\", \"ObamaCare\"]\n",
    "keywords_by_topic['FBI'] = [\"FBI\", \"Comey\", \"Mueller\", \"DOJ\"]\n",
    "keywords_by_topic['rivals'] = [\"Warren\", \"Bernie\", \"Sanders\", \"Biden\", \"Pete\", \"Buttegieg\", \"Beto\", \"O'Rourke\", \"Klobuchar\", \"Kamala\", \"Harris\", \"KamalaHarris\"]\n",
    "keywords_by_topic['MiddleEast'] = [\"ISIS\", \"Syria\", \"Yemen\", \"Iran\", \"Saudi\", \"Arabia\", \"Iraq\", \"Afghanistan\", \"Lebanon\", \"Israel\", \"Assad\", \"Ayatollah\", \"Palestine\", \"Turkey\", \"Abu\", \"al-Baghdadi\", \"Kurd\", \"Kurds\"]\n",
    "keywords_by_topic['Ukraine_scandal'] = [\"Ukraine\", \"Zelensky\", \"Schiff\", \"RepAdamSchiff\", \"impeach\", \"impeachment\", \"whistleblower\", \"Whistleblower\", \"transcript\", \"Transcript\", \"TRANSCRIPT\"]\n",
    "keywords_by_topic['economy'] = ['economy', \"Economy\",\"Economic\", \"JOBS\", \"jobs\", \"tax\", \"taxation\", \"Tax\", \"taxes\", \"growth\", \"money\", \"dollars\", \"Dollars\", \"companies\"]\n",
    "keywords_by_topic['disaster'] = [\"FEMA\", \"disaster\", \"Hurricane\", \"hurricane\"]\n",
    "\n",
    "all_keywords = []\n",
    "\n",
    "for topic in keywords_by_topic.keys():\n",
    "    print(f\"    {len(nouns[nouns['word'].isin(keywords_by_topic[topic])])} nouns about {topic}\")\n",
    "    all_keywords += keywords_by_topic[topic]\n",
    "          \n",
    "unassigned_nouns = nouns[~nouns['word'].isin(all_keywords)]\n",
    "print(f\"there are {len(unassigned_nouns)} unassigned nouns\")\n",
    "freq_analysis(unassigned_nouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.05% categorized\n",
      "2.25% broken\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv('cleaned_tweets')\n",
    "num_categorized = 0\n",
    "num_nan = 0\n",
    "\n",
    "for index, tweet in tweets.iterrows():\n",
    "    categorized = False\n",
    "    try: \n",
    "        words = nltk.word_tokenize(tweet['clean_text'])\n",
    "        for word in words:\n",
    "            if categorized == True:\n",
    "                num_categorized += 1\n",
    "                break\n",
    "            if word in all_keywords:\n",
    "                categorized = True\n",
    "                tweets.loc[index, 'categorized'] = True\n",
    "\n",
    "    except:\n",
    "        num_nan += 1\n",
    "            \n",
    "        \n",
    "        \n",
    "print(f\"{num_categorized/len(tweets)*100:.2f}% categorized\")\n",
    "print(f\"{num_nan/len(tweets)*100:.2f}% broken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retweet RepGregPence: After seven weeks of running a communist-style ultra-partisan effort to undo the 2016 election results under the disguis…\n",
      "Retweet Chief_Duggan: .POTUS recognizing theiacp Officer of the Year finalists for their heroism and  commitment to keeping our communities s…\n",
      "A great book by a great guy. Get it now! \n",
      "Correct a total scam! \n",
      "Retweet KerriKupecDOJ: .realDonaldTrump: “Nationwide injunctions undermine our entire immigration system. It is not job of judges to impose th…\n",
      "Retweet newtgingrich: The idea that politicians need to be briefed on a military operation while young men &women are risking their lives is ri…\n",
      "So nice thank you! \n",
      "THANK YOU IACP2019! \n",
      "Thank you to MarthaRaddatz and TerryMoran for a job well done! \n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "uncategorized_tweets = tweets[tweets['categorized'] != True]\n",
    "for i, uncat_tweet in uncategorized_tweets[0:10].iterrows():\n",
    "    print(uncat_tweet['clean_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
